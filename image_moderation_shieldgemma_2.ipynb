{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-dCLO4SsMdv"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs the Hugging Face `transformers` library using a \"quiet\" mode, which reduces the amount of text output you see during the installation.\n",
        "\n",
        "\n",
        "* `!`: The exclamation mark at the beginning tells a development environment, like a Jupyter notebook or Google Colab, to run the line as a **shell command** rather than as a line of Python code.\n",
        "* `pip install`: This is the core command for **pip**, the standard package installer for Python. It tells the system to find, download, and install a specific software package.\n",
        "* `-q`: This flag stands for **quiet**. It prevents the terminal from printing the large amount of informational messages that normally appear during an installation, showing only essential output like errors or warnings.\n",
        "* `transformers`: This is the name of the Python library being installed. The **Hugging Face `transformers` library** is an essential toolkit for machine learning, providing access to thousands of pre-trained models for tasks involving text, images, and audio, especially in Natural Language Processing (NLP)."
      ],
      "metadata": {
        "id": "S7Q1GL6FcOYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, ShieldGemma2ForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ],
      "metadata": {
        "id": "6a_QRDJe75Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This script begins by importing several key Python libraries required for the task:\n",
        "\n",
        "* `from transformers import AutoProcessor, ShieldGemma2ForImageClassification`: This imports two components from the **Hugging Face `transformers` library**.\n",
        "    * `ShieldGemma2ForImageClassification` is the specific **machine learning model** being used, which is a version of Google's Gemma-2 model designed for classifying images.\n",
        "    * `AutoProcessor` is a convenient tool that automatically loads the correct data pre-processor for the model, ensuring images are in the right format and size before being fed into the model.\n",
        "\n",
        "* `from PIL import Image`: This imports the `Image` module from the **Pillow library** (`PIL`). Pillow is the standard library in Python for opening, manipulating, and saving images.\n",
        "\n",
        "* `import requests`: This imports a popular library for making **HTTP requests**, which is essential for downloading data, such as images, from a URL.\n",
        "\n",
        "* `import torch`: This imports **PyTorch**, a leading open-source machine learning framework that the `transformers` library is built upon. It provides the core functionalities for tensor computations and running models on GPUs.\n",
        "\n",
        "* `import gradio as gr`: This imports the **Gradio library** with the alias `gr`. Gradio is used to quickly build and share simple web UIs for machine learning models, making them easy to test and demonstrate.\n",
        "\n",
        "* `import os`: This imports Python's built-in **operating system** module, which allows the code to interact with the OS, including setting environment variables.\n",
        "\n",
        "Finally, the line `os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"` sets an environment variable to activate **`hf-transfer`**. This is a high-performance, Rust-based utility that significantly accelerates the download speed of models and datasets from the Hugging Face Hub, which is especially useful for very large models."
      ],
      "metadata": {
        "id": "25q-ztaLcqHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    model = \"google/shieldgemma-2-4b-it\"\n",
        "    device = 'cuda'"
      ],
      "metadata": {
        "id": "olrx8KN-Tq8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a configuration class named `CFG` to store important settings for a machine learning script in a single, organized place.\n",
        "\n",
        "* `model = \"google/shieldgemma-2-4b-it\"`: This line sets the **model identifier**. The string `\"google/shieldgemma-2-4b-it\"` tells the `transformers` library to download and use a specific model from the Hugging Face Hub. In this case, it's the **ShieldGemma-2** model from Google, which has 4 billion parameters and is instruction-tuned.\n",
        "\n",
        "* `device = 'cuda'`: This line specifies the **hardware device** on which the model will run. Setting the device to `'cuda'` tells the script to use a **NVIDIA GPU** for all calculations. Running a model on a GPU is significantly faster than using a CPU. GPU. ðŸš€"
      ],
      "metadata": {
        "id": "9WAZBCOCdL4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ShieldGemma2ForImageClassification.from_pretrained(CFG.model, device_map = CFG.device)\n",
        "processor = AutoProcessor.from_pretrained(CFG.model)"
      ],
      "metadata": {
        "id": "AYN61iWf0EKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the pre-trained ShieldGemma-2 model and its associated data processor from the Hugging Face Hub.\n",
        "\n",
        "\n",
        "* `model = ShieldGemma2ForImageClassification.from_pretrained(CFG.model, device_map=CFG.device)`: This line initializes the **machine learning model**.\n",
        "    * The `from_pretrained()` method is a key feature of the `transformers` library. It automatically downloads the model weights and configuration from the Hugging Face Hub using the identifier stored in `CFG.model` (`\"google/shieldgemma-2-4b-it\"`).\n",
        "    * The `device_map=CFG.device` argument tells the library to load the model directly onto the specified hardware, which in this case is the NVIDIA **GPU** (`'cuda'`). This ensures all computations will be performed on the faster hardware.\n",
        "* `processor = AutoProcessor.from_pretrained(CFG.model)`: This line loads the **data processor** that corresponds to the model.\n",
        "    * For an image model, the processor is responsible for converting an input image into the numerical format (a tensor) that the model can understand. It handles tasks like resizing, normalizing pixel values, and structuring the data correctly.\n",
        "    * Using `AutoProcessor.from_pretrained()` with the same model identifier guarantees that the pre-processing steps are perfectly matched to the model's requirements."
      ],
      "metadata": {
        "id": "Uqv7Ub617SFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(image, policies, policy_descriptions):\n",
        "  policies = policies.split(\";\")\n",
        "  policy_descriptions = policy_descriptions.split(\";\")\n",
        "  custom_policies = dict(zip(policies, policy_descriptions))\n",
        "\n",
        "  inputs = processor(\n",
        "      images=[image], custom_policies=custom_policies,\n",
        "      policies=policies, return_tensors=\"pt\",\n",
        "  ).to(model.device)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "      output = model(**inputs)\n",
        "\n",
        "\n",
        "  outs = {}\n",
        "\n",
        "  for idx, policy in enumerate(output.probabilities.cpu()):\n",
        "    yes_prob = policy[0]\n",
        "    outs[f\"Yes for {policies[idx]}\"] = yes_prob\n",
        "\n",
        "  return outs"
      ],
      "metadata": {
        "id": "amuEF3qT7Rn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python function, named `infer`, is designed to perform inference using the `ShieldGemma-2` model. It takes an image and a set of custom rules (policies) and calculates the probability that the image complies with each rule.\n",
        "\n",
        "1.  **Parse Inputs**:\n",
        "    The function first takes two strings, `policies` and `policy_descriptions`, which are expected to contain items separated by semicolons. It splits these strings into lists and then uses `zip` to combine them into a single Python **dictionary** called `custom_policies`. This dictionary maps each policy name to its description.\n",
        "\n",
        "2.  **Pre-process Data**:\n",
        "    Next, it uses the `processor` object (loaded in a previous step) to prepare the inputs for the model. The processor takes the input `image`, the `custom_policies` dictionary, and the list of `policies`. It converts the image into the correct numerical format (a tensor) and bundles it with the policy information. The `return_tensors=\"pt\"` argument ensures the output is a PyTorch tensor, and `.to(model.device)` moves this data to the **GPU** (`'cuda'`) to match the model's location.\n",
        "\n",
        "3.  **Run Inference**:\n",
        "    This step runs the actual prediction.\n",
        "    * `with torch.inference_mode():` is a performance optimization. It tells PyTorch that no training is occurring, which makes the computation faster and more memory-efficient.\n",
        "    * `output = model(**inputs)` feeds the processed inputs into the `model`. The `**` syntax unpacks the `inputs` dictionary into the arguments the model expects. The model then evaluates the image against each policy.\n",
        "\n",
        "4.  **Format the Output**:\n",
        "    Finally, the function creates a clean, human-readable output. It initializes an empty dictionary `outs`. It then loops through the `output.probabilities` from the model, moving them back to the CPU with `.cpu()`. For each policy, it extracts the probability for the \"Yes\" case (e.g., the probability that the image *does* comply with the policy) and adds it to the `outs` dictionary with a descriptive key like `\"Yes for [policy_name]\"`.\n",
        "\n",
        "The function returns the `outs` dictionary, which contains the final probability scores for each policy."
      ],
      "metadata": {
        "id": "CBANmctP70gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      image = gr.Image(type=\"pil\")\n",
        "      policies = gr.Textbox(label = \"Names of policies - separated by semicolon\")\n",
        "      policy_descriptions = gr.Textbox(label = \"Descriptions of policies - separated by semicolon\", lines=10)\n",
        "      btn = gr.Button(\"Submit\")\n",
        "    with gr.Column():\n",
        "      outputs = gr.Label()\n",
        "\n",
        "    btn.click(fn=infer, inputs=[image, policies, policy_descriptions], outputs=outputs)\n",
        "\n",
        "  gr.Examples(examples = [  [\"Sex ; Violence \",\n",
        "                           \"The image shall not contain content that depicts explicit or graphic sexual acts.; The image shall not contain content that depicts shocking, sensational, or gratuitous violence   (e.g., excessive blood and gore, gratuitous violence against animals, extreme injury or moment of death).\"],\n",
        "\n",
        "\n",
        "                              [\"Sex ; Violence \",\n",
        "                           \"\"\"The image shall not contain content that depicts explicit or graphic sexual acts (e.g., pornography, erotic content, sexual intercourse, genital exposure, or depictions of rape or sexual assault).  Classical art, educational imagery, or non-sexualized nudityâ€”such as depictions found in the Sistine Chapel or anatomy textbooksâ€”are not considered sexually explicit for moderation purposes;  The image shall not contain content that depicts or glorifies graphic violence or physical harm (e.g., torture, murder, dismemberment, or depictions of extreme bodily injury). Images related to historical events, war documentation, or journalistic reporting that are not gratuitous or sensationalized are generally acceptable unless they focus on explicit gore or suffering.\"\"\"],\n",
        "\n",
        "                          ],\n",
        "              inputs = [policies, policy_descriptions])"
      ],
      "metadata": {
        "id": "uSqxoJnP7PQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the **Gradio library** (`gr`) to build a web user interface (UI) for the image classification model. It creates an interactive demo where a user can upload an image, define safety policies, and see the model's evaluation.\n",
        "\n",
        "***\n",
        "\n",
        "## UI Layout and Components\n",
        "\n",
        "The interface is constructed using `gr.Blocks`, which allows for a custom layout of components.\n",
        "\n",
        "* `with gr.Row():` creates the main horizontal layout, which is split into two columns.\n",
        "* **Left Column (Inputs)**: This column contains all the user input fields.\n",
        "    * `gr.Image(type=\"pil\")`: An **image upload box**. `type=\"pil\"` ensures the uploaded file is converted into a format the model's processor can handle.\n",
        "    * `gr.Textbox()`: Two **text boxes** are created. The first is for short policy names (e.g., \"Violence\"), and the second, larger box (`lines=10`) is for their detailed descriptions.\n",
        "    * `gr.Button(\"Submit\")`: A clickable **button** to start the analysis.\n",
        "* **Right Column (Outputs)**: This column is for displaying the results.\n",
        "    * `gr.Label()`: A **label component** that is designed to neatly display classification results, which will show the \"Yes\" probability for each policy.\n",
        "\n",
        "***\n",
        "\n",
        "## Interactivity\n",
        "\n",
        "The line `btn.click(fn=infer, inputs=[image, policies, policy_descriptions], outputs=outputs)` is the core of the UI's functionality. It connects the components:\n",
        "\n",
        "* When the `Submit` button (`btn`) is clicked...\n",
        "* ...it calls the `infer` function...\n",
        "* ...using the content from the `image`, `policies`, and `policy_descriptions` components as input...\n",
        "* ...and sends the function's returned dictionary to the `outputs` label for display.\n",
        "\n",
        "***\n",
        "\n",
        "## Pre-filled Examples\n",
        "\n",
        "Finally, `gr.Examples(...)` adds a section at the bottom of the UI. This provides users with pre-written examples for the policy and description fields, making it easy to test the application without having to type out long safety policies manually. Users can simply click an example to populate the fields and then upload an image to test."
      ],
      "metadata": {
        "id": "Q3GIYh4T8gIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "bgathYcS7UCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command starts the web server for the Gradio application, making the user interface accessible in a web browser.\n",
        "\n",
        "***\n",
        "\n",
        "## Method Breakdown\n",
        "\n",
        "* `demo.launch()`: This is the standard method used to run any Gradio interface. When called, it starts a local web server and provides a URL (usually `http://127.0.0.1:7860`) that you can open in your browser to interact with the UI you've built.\n",
        "\n",
        "* `debug=True`: This argument enables **Gradio's debug mode**. When `debug=True`, any errors that occur while the application is running will be printed directly in your browser. This is incredibly useful for developers to find and fix issues with the code, such as problems within the `infer` function. ðŸš€"
      ],
      "metadata": {
        "id": "qU4CLg5B855W"
      }
    }
  ]
}